{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Data EDA\n",
    "\n",
    "This notebook computes summary statistics and performs sanity checks on the relation data processing pipeline (raw_data -> relations database and sentences tagged with key terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/mattboggess/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import stanfordnlp\n",
    "import pandas as pd\n",
    "from spacy_stanfordnlp import StanfordNLPLanguage\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "# nlp preprocessing pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "snlp = stanfordnlp.Pipeline(lang=\"en\")\n",
    "nlp = StanfordNLPLanguage(snlp)\n",
    "\n",
    "# fix for importing utils\n",
    "module_path = os.path.abspath(os.path.join('../data_processing'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_processing_utils import read_spacy_docs, tag_terms\n",
    "\n",
    "data_dir = '../data/relation_extraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What concepts actually match the text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_sentences = read_spacy_docs(\"../data/preprocessed_data/Biology_2e_sentences_spacy\", nlp)\n",
    "bio_sentences += read_spacy_docs(\"../data/preprocessed_data/Life_Biology_sentences_spacy\", nlp)\n",
    "kb_terms = read_spacy_docs(\"../data/preprocessed_data/Life_Biology_kb_key_terms_spacy\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging sentence 0/44162\n",
      "tagging sentence 500/44162\n",
      "tagging sentence 1000/44162\n",
      "tagging sentence 1500/44162\n",
      "tagging sentence 2000/44162\n",
      "tagging sentence 2500/44162\n",
      "tagging sentence 3000/44162\n",
      "tagging sentence 3500/44162\n",
      "tagging sentence 4000/44162\n",
      "tagging sentence 4500/44162\n",
      "tagging sentence 5000/44162\n",
      "tagging sentence 5500/44162\n",
      "tagging sentence 6000/44162\n",
      "tagging sentence 6500/44162\n",
      "tagging sentence 7000/44162\n",
      "tagging sentence 7500/44162\n",
      "tagging sentence 8000/44162\n",
      "tagging sentence 8500/44162\n",
      "tagging sentence 9000/44162\n",
      "tagging sentence 9500/44162\n",
      "tagging sentence 10000/44162\n",
      "tagging sentence 10500/44162\n",
      "tagging sentence 11000/44162\n",
      "tagging sentence 11500/44162\n",
      "tagging sentence 12000/44162\n",
      "tagging sentence 12500/44162\n",
      "tagging sentence 13000/44162\n",
      "tagging sentence 13500/44162\n",
      "tagging sentence 14000/44162\n",
      "tagging sentence 14500/44162\n",
      "tagging sentence 15000/44162\n",
      "tagging sentence 15500/44162\n",
      "tagging sentence 16000/44162\n",
      "tagging sentence 16500/44162\n",
      "tagging sentence 17000/44162\n",
      "tagging sentence 17500/44162\n",
      "tagging sentence 18000/44162\n",
      "tagging sentence 18500/44162\n",
      "tagging sentence 19000/44162\n",
      "tagging sentence 19500/44162\n",
      "tagging sentence 20000/44162\n",
      "tagging sentence 20500/44162\n",
      "tagging sentence 21000/44162\n",
      "tagging sentence 21500/44162\n",
      "tagging sentence 22000/44162\n",
      "tagging sentence 22500/44162\n",
      "tagging sentence 23000/44162\n",
      "tagging sentence 23500/44162\n",
      "tagging sentence 24000/44162\n",
      "tagging sentence 24500/44162\n",
      "tagging sentence 25000/44162\n",
      "tagging sentence 25500/44162\n",
      "tagging sentence 26000/44162\n",
      "tagging sentence 26500/44162\n",
      "tagging sentence 27000/44162\n",
      "tagging sentence 27500/44162\n",
      "tagging sentence 28000/44162\n",
      "tagging sentence 28500/44162\n",
      "tagging sentence 29000/44162\n",
      "tagging sentence 29500/44162\n",
      "tagging sentence 30000/44162\n",
      "tagging sentence 30500/44162\n",
      "tagging sentence 31000/44162\n",
      "tagging sentence 31500/44162\n",
      "tagging sentence 32000/44162\n",
      "tagging sentence 32500/44162\n",
      "tagging sentence 33000/44162\n",
      "tagging sentence 33500/44162\n",
      "tagging sentence 34000/44162\n",
      "tagging sentence 34500/44162\n",
      "tagging sentence 35000/44162\n",
      "tagging sentence 35500/44162\n",
      "tagging sentence 36000/44162\n",
      "tagging sentence 36500/44162\n",
      "tagging sentence 37000/44162\n",
      "tagging sentence 37500/44162\n",
      "tagging sentence 38000/44162\n",
      "tagging sentence 38500/44162\n",
      "tagging sentence 39000/44162\n",
      "tagging sentence 39500/44162\n",
      "tagging sentence 40000/44162\n",
      "tagging sentence 40500/44162\n",
      "tagging sentence 41000/44162\n",
      "tagging sentence 41500/44162\n",
      "tagging sentence 42000/44162\n",
      "tagging sentence 42500/44162\n",
      "tagging sentence 43000/44162\n",
      "tagging sentence 43500/44162\n",
      "tagging sentence 44000/44162\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count occurrences of all terms\n",
    "term_counts = Counter()\n",
    "for i, sentence in enumerate(bio_sentences):\n",
    "    if i % 500 == 0:\n",
    "        print(f\"tagging sentence {i}/{len(bio_sentences)}\")\n",
    "    _, _, term_info = tag_terms(sentence, kb_terms, nlp)\n",
    "    term_counts.update({t: len(term_info[t][\"indices\"]) for t in term_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3963/5941 bio kb concepts tagged in text\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/preprocessed_data/Life_Biology_kb_lexicon.json\", \"r\") as f:\n",
    "    lexicon = json.load(f)\n",
    "\n",
    "tagged_concepts = []\n",
    "non_tagged_concepts = []\n",
    "for concept in lexicon.keys():\n",
    "    if any([lemma in set(term_counts.keys()) for lemma in lexicon[concept][\"lemma_representations\"]]) or \\\n",
    "       any([text in set(term_counts.keys()) for text in lexicon[concept][\"text_representations\"]]):\n",
    "        tagged_concepts.append({concept: lexicon[concept]})\n",
    "    else:\n",
    "        non_tagged_concepts.append({concept: lexicon[concept]})\n",
    "print(f\"{len(tagged_concepts)}/{len(non_tagged_concepts) + len(tagged_concepts)} bio kb concepts tagged in text\")\n",
    "\n",
    "df = {\"concept\": [], \"text\": []}\n",
    "for concept in non_tagged_concepts:\n",
    "    c = list(concept.keys())[0]\n",
    "    df[\"concept\"].append(c)\n",
    "    df[\"text\"].append(concept[c][\"text_representations\"])\n",
    "pd.DataFrame(df).to_excel(\"../data/relation_extraction/diagnostics/not_tagged_concepts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations Database EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/relation_extraction/relations_db.json\", \"r\") as f:\n",
    "    rdb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = {\"relation\": [], \"term_pair\": [], \"count_sentences\": [], \"found_sentence\": []}\n",
    "for relation in rdb:\n",
    "    for term_pair in rdb[relation]: \n",
    "        long_df[\"relation\"].append(relation)\n",
    "        long_df[\"term_pair\"].append(term_pair)\n",
    "        long_df[\"count_sentences\"].append(len(rdb[relation][term_pair][\"sentences\"]))\n",
    "        long_df[\"found_sentence\"].append(len(rdb[relation][term_pair][\"sentences\"]) > 0)\n",
    "long_df = pd.DataFrame(long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term_pair</th>\n",
       "      <th colspan=\"2\" halign=\"left\">count_sentences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relation</th>\n",
       "      <th>found_sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">abuts</td>\n",
       "      <td>False</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>146</td>\n",
       "      <td>5.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">element</td>\n",
       "      <td>False</td>\n",
       "      <td>443</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>92</td>\n",
       "      <td>629</td>\n",
       "      <td>6.836957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">has-part</td>\n",
       "      <td>False</td>\n",
       "      <td>5704</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>863</td>\n",
       "      <td>6553</td>\n",
       "      <td>7.593279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">has-region</td>\n",
       "      <td>False</td>\n",
       "      <td>2817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>234</td>\n",
       "      <td>1405</td>\n",
       "      <td>6.004274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">is-at</td>\n",
       "      <td>False</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "      <td>331</td>\n",
       "      <td>7.195652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">is-inside</td>\n",
       "      <td>False</td>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>156</td>\n",
       "      <td>939</td>\n",
       "      <td>6.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">is-outside</td>\n",
       "      <td>False</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>182</td>\n",
       "      <td>8.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>no-relation</td>\n",
       "      <td>True</td>\n",
       "      <td>232471</td>\n",
       "      <td>539374</td>\n",
       "      <td>2.320178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">possesses</td>\n",
       "      <td>False</td>\n",
       "      <td>1787</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>101</td>\n",
       "      <td>408</td>\n",
       "      <td>4.039604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">subclass-of</td>\n",
       "      <td>False</td>\n",
       "      <td>17937</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>1242</td>\n",
       "      <td>3918</td>\n",
       "      <td>3.154589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           term_pair count_sentences          \n",
       "                               count             sum      mean\n",
       "relation    found_sentence                                    \n",
       "abuts       False                140               0  0.000000\n",
       "            True                  26             146  5.615385\n",
       "element     False                443               0  0.000000\n",
       "            True                  92             629  6.836957\n",
       "has-part    False               5704               0  0.000000\n",
       "            True                 863            6553  7.593279\n",
       "has-region  False               2817               0  0.000000\n",
       "            True                 234            1405  6.004274\n",
       "is-at       False                258               0  0.000000\n",
       "            True                  46             331  7.195652\n",
       "is-inside   False                912               0  0.000000\n",
       "            True                 156             939  6.019231\n",
       "is-outside  False                173               0  0.000000\n",
       "            True                  22             182  8.272727\n",
       "no-relation True              232471          539374  2.320178\n",
       "possesses   False               1787               0  0.000000\n",
       "            True                 101             408  4.039604\n",
       "subclass-of False              17937               0  0.000000\n",
       "            True                1242            3918  3.154589"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = long_df.groupby([\"relation\", \"found_sentence\"]).agg({\"term_pair\": \"count\",\n",
    "                                                                  \"count_sentences\": [\"sum\", \"mean\"]})\n",
    "summary_df.to_csv(\"../data/relation_extraction/summary/relation_counts_summary.csv\", index=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Count Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95881cff3a734d4996bfdaf62cb661b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='relation', options=('no-relation', 'subclass-of', 'has-part', 'pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_sentence_counts(relation):\n",
    "    \n",
    "    df = long_df.query(f\"count_sentences > 0 & relation == '{relation}'\")\n",
    "    df = df.query(\"count_sentences < 30\")\n",
    "    plt.hist(df.count_sentences, bins=30)\n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_sentence_counts, relation=list(rdb.keys()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Label Term Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 term pairs that have multiple relations\n"
     ]
    }
   ],
   "source": [
    "term_relation_mapping = {}\n",
    "for relation in rdb:\n",
    "    if relation == \"no-relation\":\n",
    "        continue\n",
    "    for tp in rdb[relation]:\n",
    "        cp = rdb[relation][tp][\"concept_pair\"]\n",
    "        pair = \", \".join([tp, cp])\n",
    "        if pair in term_relation_mapping:\n",
    "            term_relation_mapping[pair].append(relation)\n",
    "        else:\n",
    "            term_relation_mapping[pair] = [relation]\n",
    "multi_label = {k:r for k, r in term_relation_mapping.items() if len(r) > 1}\n",
    "print(f\"Found {len(multi_label)} term pairs that have multiple relations\")\n",
    "with open(f\"../data/relation_extraction/summary/multi_label_term_pairs.json\", \"w\") as f:\n",
    "    json.dump(multi_label, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check Relation Extraction \n",
    "\n",
    "- How many word-pairs match the text on regex, but we don't match in the pipeline?\n",
    "- How many word-pairs don't match sentences? Do these seem reasonable?\n",
    "- Word-pairs with too many matches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_sentences = read_spacy_docs(\"../data/preprocessed_data/Biology_2e_sentences_spacy\", nlp)\n",
    "bio_sentences += read_spacy_docs(\"../data/preprocessed_data/Life_Biology_sentences_spacy\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-relation\n",
      "subclass-of\n",
      "Processing term pair 0/19179\n",
      "Processing term pair 100/19179\n",
      "Processing term pair 200/19179\n",
      "Processing term pair 300/19179\n",
      "Processing term pair 400/19179\n",
      "Processing term pair 500/19179\n",
      "Processing term pair 600/19179\n",
      "Processing term pair 700/19179\n",
      "Processing term pair 800/19179\n",
      "Processing term pair 900/19179\n",
      "Processing term pair 1000/19179\n",
      "Processing term pair 1100/19179\n",
      "Processing term pair 1200/19179\n",
      "Processing term pair 1300/19179\n",
      "Processing term pair 1400/19179\n",
      "Processing term pair 1500/19179\n",
      "Processing term pair 1600/19179\n",
      "Processing term pair 1700/19179\n",
      "Processing term pair 1800/19179\n",
      "Processing term pair 1900/19179\n",
      "Processing term pair 2000/19179\n",
      "Processing term pair 2100/19179\n",
      "Processing term pair 2200/19179\n",
      "Processing term pair 2300/19179\n",
      "Processing term pair 2400/19179\n",
      "Processing term pair 2500/19179\n",
      "Processing term pair 2600/19179\n",
      "Processing term pair 2700/19179\n",
      "Processing term pair 2800/19179\n",
      "Processing term pair 2900/19179\n",
      "Processing term pair 3000/19179\n",
      "Processing term pair 3100/19179\n",
      "Processing term pair 3200/19179\n",
      "Processing term pair 3300/19179\n",
      "Processing term pair 3400/19179\n",
      "Processing term pair 3500/19179\n",
      "Processing term pair 3600/19179\n",
      "Processing term pair 3700/19179\n",
      "Processing term pair 3800/19179\n",
      "Processing term pair 3900/19179\n",
      "Processing term pair 4000/19179\n",
      "Processing term pair 4100/19179\n",
      "Processing term pair 4200/19179\n",
      "Processing term pair 4300/19179\n",
      "Processing term pair 4400/19179\n",
      "Processing term pair 4500/19179\n",
      "Processing term pair 4600/19179\n",
      "Processing term pair 4700/19179\n",
      "Processing term pair 4800/19179\n",
      "Processing term pair 4900/19179\n",
      "Processing term pair 5000/19179\n",
      "Processing term pair 5100/19179\n",
      "Processing term pair 5200/19179\n",
      "Processing term pair 5300/19179\n",
      "Processing term pair 5400/19179\n",
      "Processing term pair 5500/19179\n",
      "Processing term pair 5600/19179\n",
      "Processing term pair 5700/19179\n",
      "Processing term pair 5800/19179\n",
      "Processing term pair 5900/19179\n",
      "Processing term pair 6000/19179\n",
      "Processing term pair 6100/19179\n",
      "Processing term pair 6200/19179\n",
      "Processing term pair 6300/19179\n",
      "Processing term pair 6400/19179\n",
      "Processing term pair 6500/19179\n",
      "Processing term pair 6600/19179\n",
      "Processing term pair 6700/19179\n",
      "Processing term pair 6800/19179\n",
      "Processing term pair 6900/19179\n",
      "Processing term pair 7000/19179\n",
      "Processing term pair 7100/19179\n",
      "Processing term pair 7200/19179\n"
     ]
    }
   ],
   "source": [
    "found_concepts = set()\n",
    "df = {\"sentences\": [], \n",
    "      \"relation\": [], \n",
    "      \"term-pair\": [], \n",
    "      \"concept-pair\": [], \n",
    "      \"tagged\": [], \n",
    "      \"synonym-tagged\": [], \n",
    "      \"count_sentences\": [], \n",
    "      \"textbook_match\": [],\n",
    "      \"term1_found\": [],\n",
    "      \"term2_found\": []} \n",
    "\n",
    "for relation in rdb:\n",
    "    print(relation)\n",
    "    if relation == \"no-relation\":\n",
    "        continue\n",
    "    for i, term_pair in enumerate(rdb[relation]):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing term pair {i}/{len(rdb[relation])}\")\n",
    "        \n",
    "        sentences = rdb[relation][term_pair][\"sentences\"]\n",
    "        count_sentences = len(sentences)\n",
    "        df[\"relation\"].append(relation)\n",
    "        df[\"term-pair\"].append(term_pair)\n",
    "        concept_pair = rdb[relation][term_pair][\"concept_pair\"]\n",
    "        df[\"concept-pair\"].append(concept_pair)\n",
    "        \n",
    "        if count_sentences:\n",
    "            df[\"tagged\"].append(True)\n",
    "            df[\"textbook_match\"].append(True)\n",
    "            df[\"term1_found\"].append(True)\n",
    "            df[\"term2_found\"].append(True)\n",
    "            found_concepts.add(concept_pair)\n",
    "        else:\n",
    "            terms = term_pair.split(\" -> \")\n",
    "            df[\"tagged\"].append(False)\n",
    "            found_term1 = False\n",
    "            found_term2 = False\n",
    "            found_sentences = []\n",
    "            for sentence in bio_sentences:\n",
    "                sentence = str(sentence)\n",
    "                if terms[0] in sentence:\n",
    "                    found_term1 = True\n",
    "                if terms[1] in sentence:\n",
    "                    found_term2 = True\n",
    "                if terms[0] in sentence and terms[1] in sentence:\n",
    "                    found_sentences.append(sentence)\n",
    "            sentences = found_sentences\n",
    "            df[\"term1_found\"].append(found_term1)\n",
    "            df[\"term2_found\"].append(found_term2)\n",
    "            df[\"textbook_match\"].append(len(found_sentences) > 0)\n",
    "                \n",
    "        df[\"count_sentences\"].append(count_sentences)\n",
    "        df[\"sentences\"].append(\"\\n\".join(sentences))\n",
    "                    \n",
    "for cp in df[\"concept-pair\"]:\n",
    "    df[\"synonym-tagged\"].append(cp in found_concepts)\n",
    "\n",
    "df_df = pd.DataFrame(df)\n",
    "df_df.to_excel(f\"{data_dir}/summary/relations_classifications.xlsx\")\n",
    "df_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pairs(row):\n",
    "    if row[\"tagged\"]:\n",
    "        group = \"Tagged\"\n",
    "    elif row[\"textbook_match\"]:\n",
    "        group = \"TB_Match_Not_Tagged\"\n",
    "    elif row[\"synonym-tagged\"]:\n",
    "        group = \"Synonym Tagged\"\n",
    "    elif not row[\"term1_found\"] or not row[\"term2_found\"]:\n",
    "        group = \"Missing Term\"\n",
    "    else:\n",
    "        group = \"Terms Not Same Sentence\"\n",
    "    return group\n",
    "\n",
    "df_copy = df_df.copy()\n",
    "df_copy[\"group\"] = df_copy.apply(group_pairs, axis=1)\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.groupby([\"relation\", \"group\"])[\"sentences\"].count().reset_index().rename({\"sentences\": \"count\"}, axis=1).to_csv(f\"{data_dir}/summary/relations_matches_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
